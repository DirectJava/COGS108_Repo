{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1KV7yrSZMFg"
      },
      "source": [
        "# Machine Learning in Python\n",
        "\n",
        "- Tools: scikit-learn (`sklearn`)\n",
        "    - Data Partitioning\n",
        "    - Feature selection\n",
        "    - Modeling: SVM\n",
        "    - Model Assessment\n",
        "    \n",
        " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/COGS108/Lectures-Sp25/blob/main/13_ml.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-TMF6IeZMFo"
      },
      "source": [
        "For more reading on scikit-learn (`sklearn`) and machine learning in Python: https://scikit-learn.org/stable/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PsDgUwzZMFp"
      },
      "source": [
        "# Machine Learning: General Steps\n",
        "\n",
        "1. Data Partitioning\n",
        "2. Feature Selection\n",
        "3. Model\n",
        "4. Model Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBG0bJ_1ZMFq"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kPzy7_f5ZMFq"
      },
      "outputs": [],
      "source": [
        "# import ds/plotting packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Import nltk package\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# import random for randomizing\n",
        "import random\n",
        "\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# scikit-learn imports\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t50CF-QjZMFs",
        "outputId": "a544831d-d352-4999-9a1d-1ed1a87f03f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Uncomment if you need to download the NLTK English tokenizer and the stopwords of all languages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl83blVBZMFt"
      },
      "source": [
        "# Example: Class Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awrut810ZMFt"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpcNqHJuZMFu"
      },
      "source": [
        "Student responses on COGS 108 Mid-course survey to the following two questions:\n",
        "\n",
        "- What have you enjoyed MOST about COGS 108 so far? Please explain.\n",
        "- What have you enjoyed LEAST about COGS 108 so far? Please explain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k6kB4ZUzZMFu",
        "outputId": "eddac619-a0b3-4322-dff3-6dcf6a5731df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               response  most_least quarter\n",
              "5893  Getting confused when I don't know how to solv...           0    wi25\n",
              "5894  I think it was stressful finding a group and c...           0    wi25\n",
              "5895           a few of lab assignments were repetitive           0    wi25\n",
              "5896   Project. I am not a fan of writing descriptions.           0    wi25\n",
              "5897  Probably the fact the too many assignments are...           0    wi25"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f275ae7-1247-4ba4-9c16-168f6c56d47f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>most_least</th>\n",
              "      <th>quarter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5893</th>\n",
              "      <td>Getting confused when I don't know how to solv...</td>\n",
              "      <td>0</td>\n",
              "      <td>wi25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5894</th>\n",
              "      <td>I think it was stressful finding a group and c...</td>\n",
              "      <td>0</td>\n",
              "      <td>wi25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5895</th>\n",
              "      <td>a few of lab assignments were repetitive</td>\n",
              "      <td>0</td>\n",
              "      <td>wi25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5896</th>\n",
              "      <td>Project. I am not a fan of writing descriptions.</td>\n",
              "      <td>0</td>\n",
              "      <td>wi25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5897</th>\n",
              "      <td>Probably the fact the too many assignments are...</td>\n",
              "      <td>0</td>\n",
              "      <td>wi25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f275ae7-1247-4ba4-9c16-168f6c56d47f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7f275ae7-1247-4ba4-9c16-168f6c56d47f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7f275ae7-1247-4ba4-9c16-168f6c56d47f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-def44aa6-5e70-4ed0-8252-e548665cbbfd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-def44aa6-5e70-4ed0-8252-e548665cbbfd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-def44aa6-5e70-4ed0-8252-e548665cbbfd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I think it was stressful finding a group and coordinating meeting times, but as we get further into the quarter it has gotten better. \",\n          \"Probably the fact the too many assignments are due on the same date. I think that they should be spread out.\",\n          \"a few of lab assignments were repetitive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"most_least\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quarter\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"wi25\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# read data in\n",
        "# 1 = most; 0 = least\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/shanellis/datasets/master/COGS108_ml.csv', encoding=\"ISO-8859-1\")\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h5MJCkbXZMFu",
        "outputId": "f75a66de-35be-4fa9-d377-44978697a88c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            response  most_least quarter\n",
              "0                         The idea of exploring data           1    fa20\n",
              "1                          Yes, I enjoy this class.            1    sp20\n",
              "2  The QUIZZES !! It REALLY STRESSES ME out . Alt...           0    sp21\n",
              "3  I have enjoyed getting to learn about data sci...           1    wi24\n",
              "4  IÃ¢â¬â¢ve most enjoyed the lectures. Dr Elli...           1    wi21"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41097595-9086-4575-8ae2-0e0dc88937a4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>most_least</th>\n",
              "      <th>quarter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The idea of exploring data</td>\n",
              "      <td>1</td>\n",
              "      <td>fa20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Yes, I enjoy this class.</td>\n",
              "      <td>1</td>\n",
              "      <td>sp20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The QUIZZES !! It REALLY STRESSES ME out . Alt...</td>\n",
              "      <td>0</td>\n",
              "      <td>sp21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I have enjoyed getting to learn about data sci...</td>\n",
              "      <td>1</td>\n",
              "      <td>wi24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IÃ¢â¬â¢ve most enjoyed the lectures. Dr Elli...</td>\n",
              "      <td>1</td>\n",
              "      <td>wi21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41097595-9086-4575-8ae2-0e0dc88937a4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-41097595-9086-4575-8ae2-0e0dc88937a4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-41097595-9086-4575-8ae2-0e0dc88937a4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-027ffeea-67a8-4650-976a-1fb8f929cca0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-027ffeea-67a8-4650-976a-1fb8f929cca0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-027ffeea-67a8-4650-976a-1fb8f929cca0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5898,\n  \"fields\": [\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5758,\n        \"samples\": [\n          \"The final project and group work\",\n          \"final project, hard to decide what to do and how to begin\",\n          \"maybe the quiz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"most_least\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quarter\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"sp20\",\n          \"wi25\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# randomly sort data frame\n",
        "df = df.sample(frac=1, random_state=200).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCKBYvUIZMFv"
      },
      "source": [
        "Randomly sorted data frame:\n",
        "- for selection of training and test set\n",
        "- will be approximately balanced between outcomes in each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7Hc7tOQTZMFv",
        "outputId": "223b9114-b366-4622-dafe-ce0886e37564",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5898, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# see how much data we're working with\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AS4zZPrEZMFv",
        "outputId": "bd4afc93-fc5a-41f4-c4a9-12924d909380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quarter\n",
              "fa20     551\n",
              "sp19     622\n",
              "sp20     705\n",
              "sp21     464\n",
              "wi20     488\n",
              "wi21     615\n",
              "wi24    1141\n",
              "wi25    1312\n",
              "Name: response, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quarter</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fa20</th>\n",
              "      <td>551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sp19</th>\n",
              "      <td>622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sp20</th>\n",
              "      <td>705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sp21</th>\n",
              "      <td>464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wi20</th>\n",
              "      <td>488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wi21</th>\n",
              "      <td>615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wi24</th>\n",
              "      <td>1141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wi25</th>\n",
              "      <td>1312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.groupby('quarter')['response'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWwobFS8ZMFv"
      },
      "source": [
        "### Train,  Validate, Test\n",
        "\n",
        "- Training set is what we use for creating the model\n",
        "- Validation set is what we use for fine tuning the model -- checking which hyper-parameters are best\n",
        "- Test set is what we use to figure out how good the model will do next year with new data arriving\n",
        "\n",
        "#### Method (1) - Manual split, split by quarter\n",
        "\n",
        "- We'll test the model on one quarter's responses\n",
        "- We'll validate the model on another quarter's responses\n",
        "- We'll train the model on everything else\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mMkN_t5vZMFw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "## Validation\n",
        "df_validation = df[df['quarter']=='sp20']\n",
        "\n",
        "## Test\n",
        "df_test = df[df['quarter']=='sp19']\n",
        "\n",
        "## Train\n",
        "df_train = df[~df['quarter'].isin(['sp19','sp20'])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IFe_vPHWZMFw",
        "outputId": "fb30a8cf-b1e2-4ed0-f325-5697d0b116c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4571, 3) (705, 3) (622, 3)\n"
          ]
        }
      ],
      "source": [
        "print(df_train.shape, df_validation.shape, df_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df3uQ8zEZMFw"
      },
      "source": [
        "####  Question #1\n",
        "\n",
        "How well do you think we will be able to predict whether a student comment is a response to what they liked most vs what they liked least in our ***training*** dataset?\n",
        "\n",
        "- A) Accuracy ~0%\n",
        "- B) Accuracy ~25%\n",
        "- C) Accuracy ~50%\n",
        "- D) Accuracy ~75%\n",
        "- E) Accuracy ~100%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNsPWU7fZMFw"
      },
      "source": [
        "#### Question #2\n",
        "\n",
        "How well do you think we will be able to predict whether a student comment is a response to what they liked most vs what they liked least in our ***validation*** dataset?\n",
        "\n",
        "- A) Accuracy ~0%\n",
        "- B) Accuracy ~25%\n",
        "- C) Accuracy ~50%\n",
        "- D) Accuracy ~75%\n",
        "- E) Accuracy ~100%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyu0ayO-ZMFw"
      },
      "source": [
        "#### Question #3\n",
        "\n",
        "We are testing on Sp19 data, validating on Sp20 and training data is the other quarters.  What could go wrong with splitting our dataset by quarter in this way?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwMHekwVZMFw"
      },
      "source": [
        "#### Method (2) - sklearn split, randomize the data\n",
        "- Training set 60% of the data randomly mixed across quarters\n",
        "- Validation set 20% of the data randomly mixed across quarters\n",
        "- Test set 20% of the data randomly mixed across quarters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n9jWjU0OZMFw",
        "outputId": "95b990e8-9ed4-44d0-f083-a599561bf5d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3538,) (1180,) (1180,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trainvalid, X_test, Y_trainvalid, Y_test = train_test_split(\n",
        "    df['response'], df['most_least'], test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
        "    X_trainvalid, Y_trainvalid, test_size=0.25\n",
        "\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_valid.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZzGT71kZMFx"
      },
      "source": [
        "## Prediction Task:\n",
        "\n",
        "**Classify text from students as 'most liked' or 'least liked'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBLl0vSmZMFx"
      },
      "source": [
        "#### 11 Steps to Prediction:\n",
        "\n",
        "1) Specify parameters for TF-IDF calculation<br>\n",
        "2) Specify how data will be partitioned<br>\n",
        "3) Partition the data<br>\n",
        "\n",
        "On the training data:<br>\n",
        ">4. Fit TF-IDF from text input <br>\n",
        ">5. Transform the text input to TF-IDF using the fitted TF-IDF<br>\n",
        ">6. Train model<br>\n",
        ">7. Predict in training<br>\n",
        ">8. Assess accuracy in training<br>\n",
        "\n",
        "On the test data:<br>\n",
        ">9.  Transform the text input to TF-IDF scores using the fitted TF-IDF<br>\n",
        ">10. Use model to predict in testing<br>\n",
        ">11. Assess accuracy in test set as estimate of future performance of the model<br>\n",
        "\n",
        "12) Done!<br>\n",
        "\n",
        "\n",
        "IF NEEDED: Model selection using training, validation set + test set instead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo8g_S9EZMFx"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ym3A76ZMFx"
      },
      "source": [
        "Step 1: Determine how you'll convert a collection of raw documents to a matrix of TF-IDF features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GrVTkxcWZMFy"
      },
      "outputs": [],
      "source": [
        "# Create vectorizer & specify parameters\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, #apply sublinear TF scaling\n",
        "                        analyzer='word',   #specify tokenizer\n",
        "                        max_features=500, # specify max # of features to include\n",
        "                        tokenizer=word_tokenize)\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNY6-fdrZMFy"
      },
      "source": [
        "* sublinear TF scaling - replaces term frequency (TF) with $1 + log(TF)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwydq3k7ZMFy"
      },
      "source": [
        "Step 2: Generate matrix of TF-IDF features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oF3GDzMoZMFy",
        "outputId": "bf450d4e-8250-451b-86a2-3f701e347537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-eaa5122adfc1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Learn vocabulary and idf, return term-document matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# return an array;our predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtfidf_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# take a look at the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m         )\n\u001b[0;32m-> 2104\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Learn vocabulary and idf, return term-document matrix.\n",
        "# return an array;our predictor\n",
        "tfidf_X = tfidf.fit_transform(X_train).toarray()\n",
        "\n",
        "# take a look at the output\n",
        "print(tfidf_X.shape)\n",
        "\n",
        "print(\"min: \" , np.min(tfidf_X), '\\n',\n",
        "      \"mean: \", np.mean(tfidf_X), '\\n',\n",
        "      \"max: \",  np.max(tfidf_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5uPC1TWpZMFy",
        "outputId": "2693c98a-2bc0-42ed-a288-97e3ac2e4d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tfidf_X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1fab8be92b4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf_X' is not defined"
          ]
        }
      ],
      "source": [
        "tfidf_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ovzpsf5mZMFy",
        "outputId": "4390ad8f-2e8e-4b40-8750-a29cf4e3c096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TfidfTransformer' object has no attribute 'idf_'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8cb37901162c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## get IDF to visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtoken_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36midf_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2014\u001b[0m                 \u001b[0;34m\"appropriate arguments before using this attribute.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             )\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TfidfTransformer' object has no attribute 'idf_'"
          ]
        }
      ],
      "source": [
        "## get IDF to visualize\n",
        "idf = tfidf.idf_\n",
        "rr = dict(zip(tfidf.fit(X_train).get_feature_names(), idf))\n",
        "\n",
        "token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\n",
        "token_weight.columns=('token','weight')\n",
        "token_weight = token_weight.sort_values(by='weight', ascending=False)\n",
        "token_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1PGa-gZ8ZMFz",
        "outputId": "123e714c-0fc1-45a2-bc64-e58eb65c2b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'token_weight' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-967d7decf5c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m sns.barplot(x='token', \n\u001b[1;32m      2\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             color=\"gray\")            \n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inverse Document Frequency(IDF) per token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'token_weight' is not defined"
          ]
        }
      ],
      "source": [
        "sns.barplot(x='token',\n",
        "            y='weight',\n",
        "            data=token_weight[0:10],\n",
        "            color=\"gray\")\n",
        "plt.title(\"Inverse Document Frequency(IDF) per token\")\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(15,5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA6YpQ08ZMFz"
      },
      "source": [
        "Step 3: Extract outcome variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI3LWWwfZMFz"
      },
      "outputs": [],
      "source": [
        "# specify outcome variable\n",
        "tfidf_Y = np.array(Y_train)\n",
        "tfidf_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63d54JpJZMFz"
      },
      "outputs": [],
      "source": [
        "tfidf_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V_m1UJFZMFz"
      },
      "outputs": [],
      "source": [
        "tfidf_X.shape, tfidf_Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsBYou4YZMFz"
      },
      "source": [
        "#### Clicker Question #3\n",
        "\n",
        "Looking at the code above and thinking about what we've done so far in this analysis, what is stored in `tfidf_Y`?\n",
        "\n",
        "- A) predictor variable - training data\n",
        "- B) outcome variable - training data\n",
        "- C) predictor variable - test data\n",
        "- D) outcome variable - test data\n",
        "- E) validation DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOPNDnXBZMFz"
      },
      "source": [
        "# Avoiding data leakage with feature selection\n",
        "\n",
        "NEVER do feature selection or feature transformation fitting on ALL of the data.\n",
        "\n",
        "You don't want information about your validation and test set inside your training set.\n",
        "\n",
        "If you fit your features on the whole dataset you have implictly told the training set information about the vocuabulary and frequency of things that aren't in the training set! BAD!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NZgnscDZMF8"
      },
      "outputs": [],
      "source": [
        "tfidf_train = tfidf.fit_transform(X_train)\n",
        "tfidf_valid = tfidf.transform(X_valid)\n",
        "tfidf_test = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIyIpIoaZMF9"
      },
      "outputs": [],
      "source": [
        "tfidf_train.shape, tfidf_valid.shape, tfidf_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS1r8NlpZMF9"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALiyv_2lZMF9"
      },
      "source": [
        "### SVM: Support Vector Machines\n",
        "\n",
        "- simple & interpretable machine learning model\n",
        "- based in linear regression\n",
        "- classification task\n",
        "- supervized\n",
        "    - input: labeled training data\n",
        "    - model determines hyperplane that best discriminates between categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx4lnmBxZMF9"
      },
      "source": [
        "### SVM: Tuning Parameters\n",
        "- **regularization** parameter\n",
        "    - can determine how this line is drawn\n",
        "    - can increase accuracy of prediction\n",
        "    - can lead to overfitting of the data\n",
        "- **kernel** parameter\n",
        "    - specifies how to model & transform data\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBlWSBRmZMF-"
      },
      "source": [
        "For more reading on SVMs using `sklearn`: https://scikit-learn.org/stable/modules/svm.html\n",
        "\n",
        "OR just scroll to the very end of this notebook where we get a bit crazy with them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eakGS5igZMF-"
      },
      "source": [
        "### Model Generation\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI57t0SRZMF-"
      },
      "source": [
        "Step 6: Generate and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlQdvScgZMF-"
      },
      "outputs": [],
      "source": [
        "# uncomment to read documentation for model\n",
        "SVC?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvH1TJisZMF-"
      },
      "outputs": [],
      "source": [
        "# function we'll use to run the model\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "type(svm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbsvtFiFZMF-"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "svm_model = svm_model.fit(tfidf_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF940ZHZZMF_"
      },
      "outputs": [],
      "source": [
        "svm_model.coef_.todense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT4I7UFWZMF_"
      },
      "source": [
        "### Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOOXRoWwZMF_"
      },
      "source": [
        "Step 7: Predict in the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PPUmoyXZMF_"
      },
      "outputs": [],
      "source": [
        "# predict on training\n",
        "df_predicted_train_Y = svm_model.predict(tfidf_train)\n",
        "\n",
        "print(df_predicted_train_Y[0:5])\n",
        "print(Y_train[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98mzo3QXZMF_"
      },
      "outputs": [],
      "source": [
        "# see how many were predicted most vs. least\n",
        "pd.Series(df_predicted_train_Y).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZNxwdD3ZMGA"
      },
      "source": [
        "### Testing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbc5v9AwZMGA"
      },
      "source": [
        "Step 8: Predict in the testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay9-wNkrZMGA"
      },
      "outputs": [],
      "source": [
        "# predict on training\n",
        "df_predicted_test_Y = svm_model.predict(tfidf_test)\n",
        "print(df_predicted_test_Y[0:5])\n",
        "print(Y_test[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OxJia7_ZMGA"
      },
      "outputs": [],
      "source": [
        "# see how many were predicted most vs. least\n",
        "pd.Series(df_predicted_test_Y).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXoMCTIfZMGA"
      },
      "source": [
        "## Accuracy Assessment\n",
        "\n",
        "- RMSE (continuous)\n",
        "- Accuracy, Sensitivity, Specificity, AUC\n",
        "    - TP, TN, FP, FN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WYlelTIZMGA"
      },
      "source": [
        "![confusion matrix](https://miro.medium.com/max/924/1*7EYylA6XlXSGBCF77j_rOA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gs-DjR6ZMGB"
      },
      "source": [
        "![sensitivity recell](https://miro.medium.com/max/878/1*Ub0nZTXYT8MxLzrz0P7jPA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88rrGK50ZMGB"
      },
      "source": [
        "**Accuracy** - What % were predicted correctly?  \n",
        "**Sensitivity (Recall)** - Of those that were positives, what % were predicted to be positive?  ; $\\frac {TP}{(TP + FN)}$  \n",
        "**Specificity** - Of those that were actually negatives, what % were predicted to be negative?  $\\frac {TN}{(TN + FP)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skh-9m0cZMGB"
      },
      "source": [
        "**Precision (Positive Predictive Value, PPV)** = $\\frac {TP}{(TP + FP)}$\n",
        "\n",
        "- probability that predicted positive truly is positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N6lAT-rZMGB"
      },
      "source": [
        "## A short EC quiz checkin\n",
        "\n",
        "\n",
        "Click this link:\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLScI2vt2rW7rjRVQXKEQLuC68RICLUXa-Vyo5TXcPYtGTW7aHw/viewform?usp=sharing&ouid=101992505427251219835\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1-pi3JwZMGB"
      },
      "source": [
        "### Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbMWRIjEZMGB"
      },
      "source": [
        "Step 9: Assess accuracy in training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3wr3n3HZMGB"
      },
      "outputs": [],
      "source": [
        "print(classification_report(Y_train, df_predicted_train_Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QFbTEZdZMGB"
      },
      "outputs": [],
      "source": [
        "# where 'support' comes from\n",
        "pd.Series(Y_train).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhL9BvEzZMGC"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(Y_train, df_predicted_train_Y)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AropfabZMGC"
      },
      "source": [
        "**support** - the number of occurrences of each class  \n",
        "**precision (PPV)** - ability of the classifier not to label a positive sample as negative  \n",
        "**recall (sensitivity)** - ability of the classifer to find all the positive samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZOSnTBTZMGC"
      },
      "source": [
        "**f1-score** - weighted harmonic mean of the precision and recall; score reaches its best value at 1 and worst score at 0  \n",
        "**macro average** - averaging the unweighted mean per label  \n",
        "**weighted average** - averaging the support-weighted mean per label  \n",
        "**micro average** - averaging the total true positives, false negatives and false positives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-dn_1MDZMGC"
      },
      "source": [
        "### Testing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vfuDhUnZMGC"
      },
      "source": [
        "Step 10 or 11: Assess accuracy in testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42hpPSh-ZMGD"
      },
      "outputs": [],
      "source": [
        "print(classification_report(Y_test, df_predicted_test_Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjtzK7JIZMGD"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(Y_test, df_predicted_test_Y)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBfLG-cyZMGD"
      },
      "source": [
        "#### Question #4\n",
        "\n",
        "Given this output, would you use this model to predict whether or not text was something someone liked or disliked about COGS 108?\n",
        "\n",
        "- A) Yes\n",
        "- B) No\n",
        "- C) Unsure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om0AZL58ZMGD"
      },
      "source": [
        "### Doing model selection with the validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-9bmQJ6ZMGD"
      },
      "source": [
        "Actually, usually BEFORE you test, you'll want to play with hyperparameters in validation to get the best result.  Fine tuning your model.\n",
        "\n",
        "\n",
        "So consider this an optional step, that usually comes before test if your model has a set of knobs and dials to twiddle.  \n",
        "\n",
        "We only did test first above to build some intuition for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGbkIQBbZMGD"
      },
      "outputs": [],
      "source": [
        "# play with the parameter C until it looks good\n",
        "svm_model = SVC(kernel='linear', C=10)\n",
        "svm_model.fit(tfidf_train, Y_train)\n",
        "df_predicted_validation_Y = svm_model.predict(tfidf_valid)\n",
        "\n",
        "# assess accuracy\n",
        "cm = confusion_matrix(Y_valid, df_predicted_validation_Y)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot();\n",
        "\n",
        "print(classification_report(Y_valid, df_predicted_validation_Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7zBDID0ZMGE"
      },
      "outputs": [],
      "source": [
        "#assuming we left the cell above with our BEST model setting\n",
        "# lets refit it with all of train+valid sets\n",
        "svm_model.fit(np.concatenate([tfidf_train.todense(), tfidf_valid.todense()]),\n",
        "              np.concatenate([Y_train, Y_valid]))\n",
        "\n",
        "# get test set predictions\n",
        "df_predicted_test_Y = svm_model.predict(tfidf_test.todense())\n",
        "\n",
        "print(classification_report(Y_test, df_predicted_test_Y))\n",
        "\n",
        "cm = confusion_matrix(Y_test, df_predicted_test_Y)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF2K6wBoZMGE"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_p06-S6ZMGE"
      },
      "source": [
        "1. 80:20 Partition\n",
        "2. Specified TF-IDF as predictor and most/least (0,1) as outcome\n",
        "3. Trained SVM linear classifier\n",
        "4. Built model on Training data\n",
        "5. Predicted in training data and on testing data\n",
        "6. Assessed overall accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyqrnbXuZMGE"
      },
      "source": [
        "### Approaches For Improvement?\n",
        "\n",
        "- Data Cleaning/Stemming\n",
        "- Different Tuning Parameters?\n",
        "- Cross-Validation?\n",
        "- Train/Test on all data OR Train only Sp20/Fa20/Wi21 (remote quarters)\n",
        "- Different Model?\n",
        "\n",
        "To start with use sklearn the EASY way... don't try to hand code things like Train-Test split or K-Fold cross-validation... read the docs and learn how to use these wonderful functions https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1PzgrkyZMGE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "results = cross_val_score(svm_model,\n",
        "                         X=np.concatenate([tfidf_train.todense(), tfidf_valid.todense()]),\n",
        "                         y=np.concatenate([Y_train, Y_valid]),\n",
        "                         cv=5)\n",
        "import numpy as np\n",
        "np.mean(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIX1dVK4ZMGF"
      },
      "source": [
        "## Feature engineering\n",
        "One thing we didn't talk about is how you need to engineer features for NON-text kinds of inputs\n",
        "\n",
        "In general this is a huge topic that I will try condense for you into two bullet points.\n",
        "\n",
        "- If your data has different kinds of continuous variables, and especially if some of those variables tend to be big numbers (distance to the moon in meters) and some tend to be small numbers (the earths mass as a fraction of the earth's mass), then you need to STANDARDIZE those two variables to have roughly the same magnitude. If you don't then the ML system may think that the large magnitude variable is more predictive than the small magnitude variable.  See this link https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
        "- If your data has categorical variables that do not have a natural order to them (red is not greater than blue), then you need to ONE-HOT encode those categorical variables.  Otherwise your ML system will think that red is greater than blue and that can make for some bad predictions.  See this link https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features\n",
        "\n",
        "OK lets talk about how to do things with the non text input..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_K1OBa4ZMGF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# lets get some 🐧 data to work with\n",
        "df = sns.load_dataset('penguins').dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAXc5nSIZMGG"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df, hue='species');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "patxbB_OZMGG"
      },
      "source": [
        "Hmm... lets take a close look at body mass vs bill length.... that seems really promising!!\n",
        "\n",
        "Let's imagine what it would be like to use k-Nearest Neighbors on only those two inputs... it might be enough to make things predict well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATy9UY21ZMGH"
      },
      "outputs": [],
      "source": [
        "# what we want to predict\n",
        "y = df['species']\n",
        "# the data we will predict from\n",
        "X = df[['bill_length_mm','body_mass_g']]\n",
        "\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22r98OjfZMGH"
      },
      "source": [
        "UGH!!! the vector difference $\\mathbf{x}_{new} -  \\mathbf{x}_{i}$ is going to be absolutely dominated by body mass... it's two orders of magnitude bigger than bill length!\n",
        "\n",
        "We may not want that!  From the scatterplot it looks like both variables are equally contibuting to understanding species.\n",
        "\n",
        "We need to normalize the data so each variable is a z-scored version of itself... that means both variables will now be on roughly the same order of magnitued (about -3 to 3).  There are in fact many diffrerent kinds of normalization that could work here!  Take a look at this list: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC5-7HluZMGH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "preproc = StandardScaler()\n",
        "\n",
        "preproc.fit(X)\n",
        "scaled_data = preproc.transform(X)\n",
        "X['bill_length_mm'] = scaled_data[:,0]\n",
        "X['body_mass_g'] = scaled_data[:,1]\n",
        "\n",
        "X\n",
        "\n",
        "# some things to note here... we could just use scaled_data instead of stuffing\n",
        "# the results back into the dataframe X... I'm just doing this so we can keep\n",
        "# the column labels intact... scaled_data is a numpy ndarray without those labels\n",
        "# sklearn will accept numpy arrays or pandas dataframes/series or any other iterable\n",
        "# as input, so use whatever you want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-JJeR87ZMGH"
      },
      "source": [
        "The thing we decided to learn above is strictly numerical.  And preprocessing the way we just described is only good for numerical data.\n",
        "\n",
        "What if we had been interested in categorical data? For categorical data inputs we usually wish to use a one-hot encoding.\n",
        "\n",
        "One-hotting a single categorical variable that has N possible categories makes a vector representation that is N long. Because one-hot assumes each category is mutually exclusive with every other category, only one of those N category holders is allowed to be non-zero.\n",
        "\n",
        "Why use one-hot encoding?  This eliminates unwanted \"fake relationships\" in categorical data that a different encoding might cause an ML system to learn.\n",
        "\n",
        "Imagine we have categories like (red, blue, green). There is no ordinal relationship here... red is not > blue, nor is blue > green.  But if we turned (red, blue, green) into a representation like (3, 2, 1) that is exactly what our ML system would learn. Even worse, it would be more likely to confuse blue and green than red and green... because blue-green is smaller than red-green.\n",
        "\n",
        "One hot eliminates this because the 3 long vectors representing each color [1, 0, 0], [0, 1,0], and [0, 0, 0] are orthogonal (unrelated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w2lugCtZMGI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "preproc = OneHotEncoder()\n",
        "\n",
        "onehot = preproc.fit_transform(df[['island','sex']])\n",
        "print(df[['island','sex']])\n",
        "print(onehot.todense())\n",
        "\n",
        "# there are 3 different islands, so 3 columns get used to encode which island\n",
        "# only 1 of those 3 columns can be \"hot\" (non-zero) for each penguin\n",
        "# likewise there are 2 different sexes recorded here, so 2 columns get used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcs0kiOLZMGI"
      },
      "source": [
        "anyway we're not going to use categorical data right now, so lets stick to the numerical data we did above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbJY0ge8ZMGI"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "pclassifier = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "pclassifier.fit(X,y)\n",
        "predicted = pclassifier.predict(X)\n",
        "\n",
        "print(classification_report(y,predicted))\n",
        "\n",
        "cm = confusion_matrix(y,predicted)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ki66Y8EZMGI"
      },
      "source": [
        "This result shows us that with just 2 variables we can get to 95% accuracy!\n",
        "\n",
        "BUT note this is TRAINING SET accuracy... something of limited use.\n",
        "\n",
        "Let's figure out how to make a training set/test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkMx6CdmZMGJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=777)\n",
        "\n",
        "print('training/test set sizes')\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "print()\n",
        "\n",
        "pclassifier = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "pclassifier.fit(X_train, y_train)\n",
        "predicted_train = pclassifier.predict(X_train)\n",
        "predicted_test = pclassifier.predict(X_test)\n",
        "\n",
        "print('training set performance')\n",
        "print(classification_report(y_train,predicted_train))\n",
        "print()\n",
        "print('test set performance')\n",
        "print(classification_report(y_test,predicted_test))\n",
        "print()\n",
        "print('test set confusion matrix')\n",
        "cm = confusion_matrix(y_test,predicted_test)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlbtuOlCZMGJ"
      },
      "source": [
        "Well what if instead of train/test, we do cross validation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcMEnszmZMGJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "print('lets do 5 fold cross validation...')\n",
        "X_xval, X_test, y_xval, y_test = train_test_split( X, y, test_size=0.3, random_state=777)\n",
        "\n",
        "print('xval/test set sizes')\n",
        "print(X_xval.shape, X_test.shape, y_xval.shape, y_test.shape)\n",
        "print()\n",
        "\n",
        "pclassifier = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "predicted_xval = cross_val_predict(pclassifier, X_xval, y_xval, cv=5)\n",
        "predicted_test = pclassifier.fit(X_xval,y_xval).predict(X_test)\n",
        "\n",
        "print('xval performance')\n",
        "print(classification_report(y_xval,predicted_xval))\n",
        "print()\n",
        "print('test set performance')\n",
        "print(classification_report(y_test,predicted_test))\n",
        "print()\n",
        "print('test set confusion matrix')\n",
        "cm = confusion_matrix(y_test,predicted_test)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hGwcveGZMGJ"
      },
      "source": [
        "Wow... but cross-validation is really useful for looking at the BEST set of hyperparameters possible... is there a way to do that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGh5eAwCZMGJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "print('lets do grid search using 5 fold cross validation... and this time lets use all the measurements not just the best two')\n",
        "X = df[['bill_length_mm','bill_depth_mm','flipper_length_mm', 'body_mass_g']]\n",
        "y = df['species']\n",
        "\n",
        "preproc = StandardScaler()\n",
        "\n",
        "preproc.fit(X)\n",
        "scaled_data = preproc.transform(X)\n",
        "X['bill_length_mm'] = scaled_data[:,0]\n",
        "X['bill_depth_mm'] = scaled_data[:,1]\n",
        "X['flipper_length_mm'] = scaled_data[:,2]\n",
        "X['body_mass_g'] = scaled_data[:,3]\n",
        "\n",
        "X_xval, X_test, y_xval, y_test = train_test_split( X, y, test_size=0.3, random_state=777)\n",
        "\n",
        "print('xval/test set sizes')\n",
        "print(X_xval.shape, X_test.shape, y_xval.shape, y_test.shape)\n",
        "print()\n",
        "\n",
        "# note we don't need to put n_neightbors here now :)\n",
        "pclassifier = KNeighborsClassifier()\n",
        "params = { 'n_neighbors':[1,3,5,7,9,11,13,15]}\n",
        "\n",
        "searcher = GridSearchCV(pclassifier, param_grid=params, cv=5)\n",
        "\n",
        "# at this point we run a bunch of cross validations\n",
        "searcher.fit(X_xval, y_xval)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqk2BS5JZMGK"
      },
      "outputs": [],
      "source": [
        "# a fitted set of cvs, one for each param set in the grid\n",
        "searcher.cv_results_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lScUQS7OZMGK"
      },
      "outputs": [],
      "source": [
        "# you can match this up with mean_test_score above\n",
        "searcher.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-8mNoCMZMGK"
      },
      "outputs": [],
      "source": [
        "# searcher, when used as a classifier, uses the best_params_ by default\n",
        "predicted_test = searcher.fit(X_xval,y_xval).predict(X_test)\n",
        "\n",
        "\n",
        "print('test set performance')\n",
        "print(classification_report(y_test,predicted_test))\n",
        "print()\n",
        "print('test set confusion matrix')\n",
        "cm = confusion_matrix(y_test,predicted_test)\n",
        "disp = ConfusionMatrixDisplay(cm)\n",
        "disp.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAL_wBM6ZMGK"
      },
      "source": [
        "## Support Vector Machines: Maximizing the *Margin*\n",
        "\n",
        "Support vector machines are another ML technique\n",
        "\n",
        "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
        "Here is an example of how this might look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X--lcg5hZMGK"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "\n",
        "xfit = np.linspace(-1, 3.5)\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
        "\n",
        "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVGU8W8iZMGK"
      },
      "source": [
        "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.\n",
        "Support vector machines are an example of such a *maximum margin* estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIVVVcR6ZMGK"
      },
      "source": [
        "### Fitting a support vector machine\n",
        "\n",
        "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data.\n",
        "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-KLzXuDZMGL"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS-POaLeZMGL"
      },
      "outputs": [],
      "source": [
        "dir(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNJn4UBhZMGL"
      },
      "source": [
        "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI-d9m6sZMGL"
      },
      "outputs": [],
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "\n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "\n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=3, facecolors='none', edgecolor='k');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXyibtuHZMGL"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
        "\n",
        "plot_svc_decision_function(model);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOkv_GrGZMGL"
      },
      "source": [
        "This is the dividing line that maximizes the margin between the two sets of points.\n",
        "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
        "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
        "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwqSWDZeZMGL"
      },
      "outputs": [],
      "source": [
        "model.support_vectors_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGBPZugNZMGM"
      },
      "source": [
        "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!\n",
        "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
        "\n",
        "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ44MQQTZMGM"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def plot_svm(N=10, ax=None):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    model = SVC(kernel='linear', C=1E10)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    ax = ax or plt.gca()\n",
        "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75, ax=ax)\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(model, ax)\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [30, 60, 120]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoF-74i5ZMGM"
      },
      "source": [
        "In the left panel, we see the model and the support vectors for just 10 training points, in the middle we see  60 training points and on the right panel 120 points.  While going from 30 to 60 caused changes, going from 60 to 120 did not becuase the support vectors (the points on the margin line) had not changed.\n",
        "\n",
        "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHjYag5QZMGM"
      },
      "source": [
        "### Beyond linear boundaries: Kernel SVM\n",
        "\n",
        "Where SVM becomes extremely powerful is when it is combined with *kernels*.\n",
        "We have seen a version of kernels before: Remember polynomial features from the regression lectures?  \n",
        "\n",
        "There we projected our data into higher-dimensional space defined by polynomials functions, and thereby were able to fit for nonlinear relationships with a linear regression.\n",
        "\n",
        "In SVM models, we can use a version of the same idea.\n",
        "To motivate the need for kernels, let's look at some data that is not linearly separable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yQUe0pTZMGM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2');\n",
        "#plot_svc_decision_function(clf, plot_support=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKrgiZVAZMGM"
      },
      "source": [
        "It is clear that no linear discrimination will *ever* be able to separate this data.\n",
        "\n",
        "One simple projection we could use would be to compute a *radial basis function* centered on the middle clump:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPzYMUuKZMGM"
      },
      "outputs": [],
      "source": [
        "phi = np.exp(-(X ** 2).sum(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf68UaSPZMGM"
      },
      "source": [
        "Adding phi as a 3rd dimension gives us this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hy-23uIZMGM"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "from mpl_toolkits import mplot3d\n",
        "from matplotlib.colors import ListedColormap\n",
        "cmap = ListedColormap(sns.color_palette()[:2])\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "ax.scatter3D(X[:, 0], X[:, 1], phi, c=y, s=7, cmap=cmap)\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_zlabel('$\\phi(\\mathbf{x})_3$');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbnuGONNZMGN"
      },
      "source": [
        "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *phi*=0.7.\n",
        "\n",
        "Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
        "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
        "\n",
        "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
        "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
        "\n",
        "A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.\n",
        "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection!\n",
        "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
        "\n",
        "In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the ``kernel`` model hyperparameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7itqnX6KZMGN"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MtQA19SZMGN"
      },
      "outputs": [],
      "source": [
        "clf = SVC(kernel='poly', C=1)\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v49P2p3LZMGN"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75)\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=300, lw=1, facecolors='none', edgecolor='k');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp7GH-S-ZMGN"
      },
      "source": [
        "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
        "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlEJzRXFZMGN"
      },
      "source": [
        "### Tuning the SVM: Softening Margins\n",
        "\n",
        "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
        "But what if your data has some amount of overlap?\n",
        "For example, you may have data like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKxDFwaxZMGO"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=1.2)\n",
        "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ugAMCTZMGO"
      },
      "source": [
        "To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
        "The hardness of the margin is controlled by a tuning parameter, most often known as $C$.\n",
        "For very large $C$, the margin is hard, and points cannot lie in it.\n",
        "For smaller $C$, the margin is softer, and can grow to encompass some points.\n",
        "\n",
        "The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2uVJ1mhZMGO"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=0.8)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for axi, C in zip(ax, [10.0, 0.1]):\n",
        "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
        "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75, ax=axi)\n",
        "    plot_svc_decision_function(model, axi)\n",
        "    axi.scatter(model.support_vectors_[:, 0],\n",
        "                model.support_vectors_[:, 1],\n",
        "                s=300, lw=1, facecolors='none', edgecolor='k');\n",
        "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyNDVmnxZMGO"
      },
      "source": [
        "The optimal value of the $C$ parameter will depend on your dataset, and should be tuned using cross-validation or a similar model selection procedure.\n",
        "\n",
        "\n",
        "In fact in general we would want to do model selection on all the hyperparameters of the SVM... $C$, what kernel to use, and whatever hyperparameters are unique to each kernel like polynomial order in $\\phi$.\n",
        "\n",
        "In fact sklearn gives us a powerful tool to do exactly this. Which I will demonstrate below.\n",
        "\n",
        "If you're actually following us all the way to this point you are getting into the territory of where I want you to be during this point in the quarter in COGS118A.  So if you are NOT following don't worry too much about it... this is advanced stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZOkkiTCZMGO"
      },
      "outputs": [],
      "source": [
        "# NOTE: this will take around 2-3 minutes to run\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "\n",
        "# what we want to predict\n",
        "y = df['species']\n",
        "# the data we will predict from... note that we will include SEX\n",
        "# because penguin sex is actually pretty important in their size measurements\n",
        "X = df.drop(columns=['island','species'])\n",
        "\n",
        "# sex is a categorical variable... it needs to be one hot encoded!\n",
        "# we know from before that we need to z-score the numeric variables\n",
        "\n",
        "# here we setup a way to transform the categorical variables into one hot,\n",
        "# and to z-score the numeric variables.... completely automatically!!\n",
        "# remember how we had to do this with a bunch of different\n",
        "# steps manually the last time around??\n",
        "# if you'd like to understand how to do this more generally\n",
        "# search the sklearn docs for ColumnTransformer!\n",
        "numerical_columns_selector = selector(dtype_exclude=object)\n",
        "categorical_columns_selector = selector(dtype_include=object)\n",
        "\n",
        "numerical_columns = numerical_columns_selector(X)\n",
        "categorical_columns = categorical_columns_selector(X)\n",
        "\n",
        "onehot = OneHotEncoder()\n",
        "scaler = StandardScaler()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('onehot', onehot, categorical_columns),\n",
        "    ('zscore', scaler, numerical_columns)])\n",
        "\n",
        "\n",
        "# Create a pipeline\n",
        "# a pipeline puts together two (or 3 or however many) steps into one sklearn\n",
        "# estimator. That means we can use pipe instead of svc in any xval or grid search\n",
        "# and get the preprocessing done for us in the same step\n",
        "pipe = Pipeline([('make_features', preprocessor),\n",
        "                 ('classifier', SVC())])\n",
        "\n",
        "# SVM have very different hyperparameters depending on which kernel\n",
        "# potential hyperparams are gamma, coef0, degree and kernel.\n",
        "# here <x,x'> and ||x-x'|| are similarities between two vectors x,x'\n",
        "# linear kernel:     <x,x'>  [no hyperparams]\n",
        "# poly kernel:       ( gamma * <x,x'> + coef0 )^degree\n",
        "# rbf kernel:        exp( - gamma * ||x-x'||^2 )\n",
        "# sigmoid kernel:    tanh( gamma * <x,x'> + coef0 )\n",
        "\n",
        "# so how can we search using only using hypers appropriate for each kernel?\n",
        "# like this...make a list of different search spaces you want to execute\n",
        "search_space = [{'classifier__kernel': ['linear'],\n",
        "                 'classifier__C': np.logspace(-3, 2, 11) #11 steps between 10^-3 to 10^2\n",
        "                },\n",
        "                {'classifier__kernel': ['poly'],\n",
        "                 'classifier__gamma': np.logspace(-3, 2, 11),\n",
        "                 'classifier__degree': range(2,14),\n",
        "                 # NOPE... this isnt often important 'classifier__coef0': np.logspace(-3, 2, 11),\n",
        "                 'classifier__C': np.logspace(-3, 2, 11)\n",
        "                },\n",
        "                {'classifier__kernel': ['rbf'],\n",
        "                 'classifier__gamma': np.logspace(-3, 2, 11),\n",
        "                 'classifier__C': np.logspace(-3, 2, 11)\n",
        "                },\n",
        "                ]\n",
        "\n",
        "\n",
        "\n",
        "# Create a grid search object to find the best model\n",
        "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=1)\n",
        "# play with different verbose=??\n",
        "\n",
        "# now lets split off some of our data for testing set... the rest will be used from cross validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.25, random_state=101)\n",
        "\n",
        "\n",
        "# Fit grid search\n",
        "%time best_model.fit(X_train, y_train)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15OfisuQZMGO"
      },
      "outputs": [],
      "source": [
        "# lets find out how well we do on the test set...\n",
        "print(best_model.best_params_)\n",
        "yhat = best_model.predict(X_test)\n",
        "print(classification_report(y_test, yhat))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, yhat);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb6gy9heZMGO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}